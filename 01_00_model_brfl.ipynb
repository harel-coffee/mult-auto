{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from scipy.stats import ks_2samp\n",
    "from model import OptimizedKMeans\n",
    "from model import GeneticProfiling, GeneticClustering\n",
    "from model import DenoisingAutoencoder\n",
    "from correlation import select_genes\n",
    "from util import to_data_frame\n",
    "from itertools import compress\n",
    "from datetime import datetime\n",
    "\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical = pd.read_csv('data/clinical_brfl.tsv', sep='\\t', index_col='ID')\n",
    "\n",
    "genefpkm = pd.read_csv('data/gene_fpkm.tsv', sep='\\t', index_col='ID')\n",
    "\n",
    "selected_index = clinical.join(genefpkm, how='inner').index\n",
    "\n",
    "clinical = clinical.loc[selected_index,:]\n",
    "\n",
    "clinical['response_best_response_first_line'] = clinical['response_best_response_first_line'].astype(int)\n",
    "\n",
    "genefpkm = genefpkm.loc[selected_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining General Classification Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting_type': 'gbdt', \n",
    "          'objective': 'binary',\n",
    "          'num_class': 1,\n",
    "          'metric': 'logloss',\n",
    "          'learning_rate': 0.01, \n",
    "          'num_leaves': 31, \n",
    "          'max_depth': 4,  \n",
    "          'min_child_samples': 20, \n",
    "          'max_bin': 255,  \n",
    "          'subsample': 0.8, \n",
    "          'subsample_freq': 0,  \n",
    "          'colsample_bytree': 0.3,  \n",
    "          'min_child_weight': 5, \n",
    "          'subsample_for_bin': 200000,\n",
    "          'min_split_gain': 0, \n",
    "          'reg_alpha': 0, \n",
    "          'reg_lambda': 0, \n",
    "          'nthread': 6, \n",
    "          'verbose': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming Qualitative Variables into Dummy Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_best_response_first_line</th>\n",
       "      <th>percent_aneuploid</th>\n",
       "      <th>percent_plama_cells_bone_marrow</th>\n",
       "      <th>percent_plama_cells_peripherical_blood</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>iss</th>\n",
       "      <th>absolute_neutrophil</th>\n",
       "      <th>platelet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MMRF1029</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.080</td>\n",
       "      <td>1</td>\n",
       "      <td>2.60</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF1030</th>\n",
       "      <td>1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.692</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF1031</th>\n",
       "      <td>0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.328</td>\n",
       "      <td>1</td>\n",
       "      <td>10.29</td>\n",
       "      <td>385.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF1032</th>\n",
       "      <td>0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.720</td>\n",
       "      <td>2</td>\n",
       "      <td>1.30</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF1033</th>\n",
       "      <td>0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.560</td>\n",
       "      <td>1</td>\n",
       "      <td>3.99</td>\n",
       "      <td>307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF1037</th>\n",
       "      <td>0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.720</td>\n",
       "      <td>1</td>\n",
       "      <td>3.20</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF1038</th>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.240</td>\n",
       "      <td>3</td>\n",
       "      <td>5.89</td>\n",
       "      <td>310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMRF1048</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>60.112</td>\n",
       "      <td>1</td>\n",
       "      <td>2.10</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          response_best_response_first_line  percent_aneuploid  \\\n",
       "ID                                                               \n",
       "MMRF1029                                  0                0.0   \n",
       "MMRF1030                                  1               15.4   \n",
       "MMRF1031                                  0               18.3   \n",
       "MMRF1032                                  0               20.7   \n",
       "MMRF1033                                  0               18.5   \n",
       "MMRF1037                                  0               20.7   \n",
       "MMRF1038                                  0               29.0   \n",
       "MMRF1048                                  0                0.0   \n",
       "\n",
       "          percent_plama_cells_bone_marrow  \\\n",
       "ID                                          \n",
       "MMRF1029                              8.4   \n",
       "MMRF1030                              9.6   \n",
       "MMRF1031                             10.1   \n",
       "MMRF1032                             11.1   \n",
       "MMRF1033                             12.0   \n",
       "MMRF1037                             17.0   \n",
       "MMRF1038                             22.0   \n",
       "MMRF1048                              9.6   \n",
       "\n",
       "          percent_plama_cells_peripherical_blood  creatinine  iss  \\\n",
       "ID                                                                  \n",
       "MMRF1029                                     0.0     106.080    1   \n",
       "MMRF1030                                     0.0      55.692    1   \n",
       "MMRF1031                                     0.0      81.328    1   \n",
       "MMRF1032                                     0.0      70.720    2   \n",
       "MMRF1033                                     0.0      79.560    1   \n",
       "MMRF1037                                     0.0      70.720    1   \n",
       "MMRF1038                                     0.0      97.240    3   \n",
       "MMRF1048                                     0.6      60.112    1   \n",
       "\n",
       "          absolute_neutrophil  platelet  \n",
       "ID                                       \n",
       "MMRF1029                 2.60     219.0  \n",
       "MMRF1030                 2.50     215.0  \n",
       "MMRF1031                10.29     385.0  \n",
       "MMRF1032                 1.30     166.0  \n",
       "MMRF1033                 3.99     307.0  \n",
       "MMRF1037                 3.20     361.0  \n",
       "MMRF1038                 5.89     310.0  \n",
       "MMRF1048                 2.10     215.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in clinical:\n",
    "    \n",
    "    values = clinical[column]\n",
    "    \n",
    "    if values.dtype == 'object':\n",
    "        \n",
    "        values = pd.get_dummies(values)\n",
    "        \n",
    "        values.columns = [column + '_' + str(c).lower().replace(' ', '_') for c in values.columns]\n",
    "    \n",
    "        del clinical[column]\n",
    "    \n",
    "        clinical = clinical.join(values, how='inner')\n",
    "\n",
    "clinical = clinical.fillna(0)\n",
    "\n",
    "clinical.iloc[:8,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "class DenoisingAutoencoder(Model):\n",
    "\n",
    "    def __init__(self, model_name=None, summaries_dir='../output/'):\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.session = tf.Session(graph=self.graph)\n",
    "\n",
    "            self.model_name = model_name\n",
    "\n",
    "            self.input = None\n",
    "\n",
    "            self.corrupted_input = None\n",
    "\n",
    "            self.learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "            self.add_summaries = summaries_dir is not None\n",
    "\n",
    "            self.summaries_dir = summaries_dir\n",
    "\n",
    "            self.keep_probability = tf.placeholder(tf.float32, name='keep_probability')\n",
    "\n",
    "            self.batch_size = None\n",
    "            \n",
    "            self.layer_index = None\n",
    "\n",
    "    def build(self, n_inputs, encoder_units=(128,), decoder_units=(128,), encoder_activation_function='sigmoid', decoder_activation_function='identity'):\n",
    "\n",
    "        assert isinstance(encoder_units, tuple) and len(encoder_units) > 0, 'encoder_units should tuple with at least one element'\n",
    "        \n",
    "        assert isinstance(decoder_units, tuple) and len(decoder_units) > 0, 'decoder_units should tuple with at least one element'\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            self.input = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name='input')\n",
    "\n",
    "            with tf.name_scope('random_noise'):\n",
    "\n",
    "                mask = tf.random_uniform(shape=tf.shape(self.input), minval=0, maxval=1, dtype=tf.float32, seed=None, name=None)\n",
    "\n",
    "                mask = tf.where(mask <= self.keep_probability, tf.ones_like(self.input, dtype=tf.float32), tf.zeros_like(self.input, dtype=tf.float32))\n",
    "\n",
    "                self.corrupted_input = tf.multiply(self.input, mask)\n",
    "\n",
    "            with tf.name_scope('encoder'):\n",
    "\n",
    "                self.encoder = self.corrupted_input\n",
    "                \n",
    "                for layer_index, units in enumerate(encoder_units):\n",
    "                \n",
    "                    name = 'last_encoder' if layer_index == len(encoder_units) - 1 else 'encoder_{}'.format(layer_index + 1)\n",
    "               \n",
    "                    self.encoder = tf.layers.dense(self.encoder, units, kernel_initializer=tf.truncated_normal_initializer(), \n",
    "                                                   kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4))\n",
    "                    \n",
    "                    self.encoder = self.get_activation_function(encoder_activation_function)(self.encoder, name=name)\n",
    "\n",
    "            with tf.name_scope('decoder'):\n",
    "\n",
    "                self.decoder = self.encoder\n",
    "                \n",
    "                for layer_index, units in enumerate(decoder_units):\n",
    "                \n",
    "                    self.decoder = tf.layers.dense(self.decoder, n_inputs, kernel_initializer=tf.truncated_normal_initializer(),\n",
    "                                                  kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4))\n",
    "\n",
    "                    self.decoder = self.get_activation_function(encoder_activation_function)(self.decoder, name='decoder_{}'.format(layer_index + 1))\n",
    "                    \n",
    "                self.decoder = tf.layers.dense(self.decoder, n_inputs, kernel_initializer=tf.truncated_normal_initializer(),\n",
    "                                               kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-4))\n",
    "\n",
    "                self.decoder = self.get_activation_function(decoder_activation_function)(self.decoder, name='last_encoder')\n",
    "\n",
    "            self.layer_index = layer_index + 1\n",
    "                \n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "    def fit(self, x, keep_probability=0.75, learning_rate=1e-4, steps=10000, batch_size=None, shuffle=True, optimizer='sgd', loss='mse'):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        assert steps > 0, 'steps should be an integer greater than zero'\n",
    "\n",
    "        assert batch_size is None or 0 < batch_size <= x.shape[0], 'bath should be none or an integer between zero (exclusive) and number of input features (inclusive)'\n",
    "\n",
    "        self.best_error = np.inf\n",
    "        \n",
    "        iterations_without_improvements = 0\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "\n",
    "            self.__build_optimizer(optimizer, loss)\n",
    "\n",
    "            test_writer, test_results = None, None\n",
    "\n",
    "            if batch_size is None:\n",
    "                batch_size = x.shape[0]\n",
    "\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "            with self.graph.as_default():\n",
    "\n",
    "                self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "                self.session.run(tf.local_variables_initializer())\n",
    "                        \n",
    "                opt_metric_value = tf.placeholder(dtype=tf.float32, name='optimization_metric_ph')\n",
    "        \n",
    "                opt_metric_value_summary = tf.summary.scalar('mean_' + loss, opt_metric_value)\n",
    "\n",
    "                if self.add_summaries:\n",
    "                    test_writer = tf.summary.FileWriter(self.summaries_dir + '/{}'.format(self.model_name), tf.get_default_graph())\n",
    "\n",
    "                n_rows = x.shape[0]\n",
    "\n",
    "                index = np.array(list(range(n_rows)), dtype=np.int)\n",
    "\n",
    "                j, logdata = 0, None\n",
    "                \n",
    "                \n",
    "                for step in range(steps):\n",
    "                    \n",
    "                    logs = []\n",
    "                    \n",
    "                    current_block = 0\n",
    "\n",
    "                    while current_block < n_rows:\n",
    "\n",
    "                        if shuffle:\n",
    "                            np.random.shuffle(index)\n",
    "\n",
    "                        batch = list(range(current_block, (min(current_block + batch_size, n_rows))))\n",
    "\n",
    "                        loss_value = self.session.run([self.optimizer, self.loss],\n",
    "                                                        feed_dict={self.input: x[index[batch], :],\n",
    "                                                                   self.learning_rate: learning_rate,\n",
    "                                                                   self.keep_probability: keep_probability})[1]\n",
    "                        \n",
    "                        logs.append(loss_value)\n",
    "                        \n",
    "                        current_block += batch_size\n",
    "\n",
    "                        j += 1\n",
    "\n",
    "                    if self.add_summaries:\n",
    "                        \n",
    "                        summary_scalar = self.session.run(opt_metric_value_summary, feed_dict={opt_metric_value: np.mean(logs)})\n",
    "                        \n",
    "                        test_writer.add_summary(summary_scalar, step)\n",
    "\n",
    "                    if step == steps - 1:\n",
    "                        self.saver.save(self.session, '{0}/{1}/graph/{1}'.format(self.summaries_dir, self.model_name), global_step=step)\n",
    "\n",
    "                    if self.best_error > np.mean(logs):\n",
    "                        \n",
    "                        iterations_without_improvements = 0\n",
    "                        \n",
    "                        self.best_error = np.mean(logs)\n",
    "                        \n",
    "                        self.saver.save(self.session, '{0}/{1}/graph/{1}__BESTONE__'.format(self.summaries_dir, self.model_name))\n",
    "                        \n",
    "                    else:\n",
    "                        iterations_without_improvements += 1\n",
    "                    \n",
    "                    if iterations_without_improvements > 1000:\n",
    "                        \n",
    "                        print('early stopping after {} iterations without improvements: best metri value {}'.format(iterations_without_improvements, np.mean(logs)))\n",
    "                        \n",
    "                        break\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = 1000\n",
    "\n",
    "        x_line = None\n",
    "\n",
    "        start, end = 0, min(self.batch_size, x.shape[0])\n",
    "\n",
    "        while start < x.shape[0]:\n",
    "\n",
    "            with self.graph.as_default():\n",
    "                x_ = self.session.run([self.decoder], feed_dict={self.input: x[start:end, :], self.keep_probability: 1.0})[0]\n",
    "\n",
    "            if x_line is None:\n",
    "                x_line = x_\n",
    "\n",
    "            else:\n",
    "                x_line = np.concatenate((x_line, x_), axis=0)\n",
    "\n",
    "            start, end = end, min(x.shape[0], end + self.batch_size)\n",
    "\n",
    "        return x_line\n",
    "\n",
    "    def encode(self, x):\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = 1000\n",
    "\n",
    "        x_line = None\n",
    "\n",
    "        start, end = 0, min(self.batch_size, x.shape[0])\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            while start < x.shape[0]:\n",
    "\n",
    "                x_ = self.session.run([self.encoder], feed_dict={self.input: x[start:end, :],\n",
    "                                                                 self.keep_probability: 1.0})[0]\n",
    "\n",
    "                if x_line is None:\n",
    "                    x_line = x_\n",
    "\n",
    "                else:\n",
    "                    x_line = np.concatenate((x_line, x_), axis=0)\n",
    "\n",
    "                start, end = end, min(x.shape[0], end + self.batch_size)\n",
    "\n",
    "        return x_line\n",
    "\n",
    "    def transform(self, x):\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = 1000\n",
    "\n",
    "        x_line = None\n",
    "\n",
    "        start, end = 0, min(self.batch_size, x.shape[0])\n",
    "\n",
    "        while start < x.shape[0]:\n",
    "\n",
    "            with self.graph.as_default():\n",
    "                x_ = self.session.run([tf.reduce_sum(tf.square(self.input - self.decoder), axis=1)],\n",
    "                                      feed_dict={self.input: x[start:end, :], self.keep_probability: 1.0})[0]\n",
    "\n",
    "            if x_line is None:\n",
    "                x_line = x_\n",
    "\n",
    "            else:\n",
    "                x_line = np.concatenate((x_line, x_), axis=0)\n",
    "\n",
    "            start, end = end, min(x.shape[0], end + self.batch_size)\n",
    "\n",
    "        return x_line\n",
    "\n",
    "    def get_error(self, x):\n",
    "\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = 1000\n",
    "\n",
    "        x_line = None\n",
    "\n",
    "        start, end = 0, min(self.batch_size, x.shape[0])\n",
    "\n",
    "        while start < x.shape[0]:\n",
    "\n",
    "            with self.graph.as_default():\n",
    "                x_ = self.session.run([self.input - self.decoder],\n",
    "                                      feed_dict={self.input: x[start:end, :], self.keep_probability: 1.0})[0]\n",
    "\n",
    "            if x_line is None:\n",
    "                x_line = x_\n",
    "\n",
    "            else:\n",
    "                x_line = np.concatenate((x_line, x_), axis=0)\n",
    "\n",
    "            start, end = end, min(x.shape[0], end + self.batch_size)\n",
    "\n",
    "        return x_line\n",
    "\n",
    "    def fit_encode(self, x, keep_probability=0.75, learning_rate=1e-4, steps=1000, batch_size=None, shuffle=True):\n",
    "\n",
    "        self.fit(x, keep_probability, learning_rate, steps, batch_size, shuffle)\n",
    "\n",
    "        return self.encode(x)\n",
    "\n",
    "    def fit_transform(self, x, keep_probability=0.75, learning_rate=1e-2, steps=1000, batch_size=None, shuffle=True):\n",
    "\n",
    "        self.fit(x, keep_probability, learning_rate, steps, batch_size, shuffle)\n",
    "\n",
    "        return self.transform(x)\n",
    "\n",
    "    def load(self, model_path):\n",
    "\n",
    "        if os.path.exists('{}.meta'.format(model_path)) and os.path.isfile('{}.meta'.format(model_path)):\n",
    "\n",
    "            with self.graph.as_default():\n",
    "\n",
    "                self.saver = tf.train.import_meta_graph('{}.meta'.format(model_path))\n",
    "\n",
    "                self.saver.restore(self.session, tf.train.latest_checkpoint(os.path.dirname(model_path)))\n",
    "\n",
    "                self.input = tf.get_default_graph().get_tensor_by_name('input:0')\n",
    "\n",
    "                self.keep_probability = tf.get_default_graph().get_tensor_by_name('keep_probability_1:0')\n",
    "\n",
    "                self.encoder = tf.get_default_graph().get_tensor_by_name('encoder/last_encoder:0')\n",
    "\n",
    "                self.decoder = tf.get_default_graph().get_tensor_by_name('decoder/last_decoder:0')\n",
    "\n",
    "    def __build_optimizer(self, optimizer, loss):\n",
    "\n",
    "        with tf.name_scope('optimization'):\n",
    "\n",
    "            with tf.name_scope('loss'):\n",
    "\n",
    "                self.loss = self.get_loss(loss)(self.input, self.decoder)\n",
    "\n",
    "                self.loss = tf.reduce_mean(self.loss)\n",
    "\n",
    "                tf.summary.scalar('dae', self.loss)\n",
    "\n",
    "            self.optimizer = self.get_optimizer(optimizer)(learning_rate=self.learning_rate)\n",
    "\n",
    "            self.optimizer = self.optimizer.minimize(self.loss, name='optimizer')\n",
    "\n",
    "        if self.add_summaries:\n",
    "            #\n",
    "            # Create summary tensors\n",
    "            #\n",
    "            self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Selecting gene expressions\n",
      "Computing genetic profling\n",
      "Computing genetic clustering\n",
      "Denoising autoencoder\n",
      "early stopping after 1001 iterations without improvements: best metri value 380812134449152.0\n",
      "\n",
      "Fold #2\n",
      "Selecting gene expressions\n",
      "Computing genetic profling\n",
      "Computing genetic clustering\n",
      "Denoising autoencoder\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "kfold = StratifiedKFold(10, random_state=13)\n",
    "\n",
    "result = None\n",
    "    \n",
    "x, y = clinical.values[:, 1:], clinical.values[:, 0]\n",
    "\n",
    "for i, (train_index, valid_index) in enumerate(kfold.split(x, y)):\n",
    "    \n",
    "    print('Fold #{}'.format(i + 1))\n",
    "    \n",
    "    #\n",
    "    # Split train & valid\n",
    "    #\n",
    "    response_train = clinical.iloc[train_index, 0]\n",
    "    response_valid = clinical.iloc[valid_index, 0]\n",
    "    \n",
    "    clinical_train = clinical.iloc[train_index, 1:]\n",
    "    clinical_valid = clinical.iloc[valid_index, 1:]\n",
    "    \n",
    "    genefpkm_train = genefpkm.iloc[train_index, :]\n",
    "    genefpkm_valid = genefpkm.iloc[valid_index, :]\n",
    "    \n",
    "    #\n",
    "    # Select gene expressions\n",
    "    #\n",
    "    print('Selecting gene expressions')\n",
    "    \n",
    "    if os.path.isfile('output/selected_genes_fold_{}.pkl'.format(i)):\n",
    "        with open('output/selected_genes_fold_{}.pkl'.format(i), 'rb') as file:\n",
    "            selected_genes = pickle.load(file)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        selected_genes = select_genes(genefpkm_train, response_train)\n",
    "        \n",
    "        with open('output/selected_genes_fold_{}.pkl'.format(i), 'wb') as file:\n",
    "            pickle.dump(selected_genes, file)\n",
    "    \n",
    "    genefpkm_train = genefpkm_train[selected_genes]\n",
    "    \n",
    "    genefpkm_valid = genefpkm_valid[selected_genes]\n",
    "    \n",
    "    #\n",
    "    # Genetic Profiling\n",
    "    #\n",
    "    print('Computing genetic profling')\n",
    "    \n",
    "    if os.path.isfile('output/kmeans_genetic_profiling_fold_{}.pkl'.format(i)):\n",
    "        \n",
    "        with open('output/kmeans_genetic_profiling_fold_{}.pkl'.format(i), 'rb') as file:\n",
    "            genetic_profiling = pickle.load(file)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        genetic_profiling = GeneticProfiling(random_state=10)\n",
    "\n",
    "        genetic_profiling.fit(genefpkm_train)\n",
    "        \n",
    "        with open('output/kmeans_genetic_profiling_fold_{}.pkl'.format(i), 'wb') as file:\n",
    "            pickle.dump(genetic_profiling, file)\n",
    "        \n",
    "    \n",
    "    profiling_train = to_data_frame(genetic_profiling.transform(genefpkm_train), prefix='PV', index=genefpkm_train.index)\n",
    "    clinical_train = pd.concat([clinical_train, profiling_train], axis=1)\n",
    "    \n",
    "    profiling_valid = to_data_frame(genetic_profiling.transform(genefpkm_valid), prefix='PV', index=genefpkm_valid.index)\n",
    "    clinical_valid = pd.concat([clinical_valid, profiling_valid], axis=1)\n",
    "    \n",
    "    #\n",
    "    # Genetic Clustering\n",
    "    #\n",
    "    print('Computing genetic clustering')\n",
    "    \n",
    "    if os.path.isfile('output/kmeans_genetic_clustering_fold_{}.pkl'.format(i)):\n",
    "        \n",
    "        with open('output/kmeans_genetic_clustering_fold_{}.pkl'.format(i), 'rb') as file:\n",
    "            genetic_clustering = pickle.load(file)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        genetic_clustering = GeneticClustering(random_state=10, verbose=0, early_stopping_rounds=10)\n",
    "\n",
    "        genetic_clustering.fit(genefpkm_train)\n",
    "        \n",
    "        with open('output/kmeans_genetic_clustering_fold_{}.pkl'.format(i), 'wb') as file:\n",
    "            pickle.dump(genetic_clustering, file)\n",
    "    \n",
    "    gene_cluster_train = to_data_frame(genetic_clustering.transform(genefpkm_train), prefix='GC', index=genefpkm_train.index)\n",
    "    clinical_train = pd.concat([clinical_train, gene_cluster_train], axis=1)\n",
    "    \n",
    "    gene_cluster_valid = to_data_frame(genetic_clustering.transform(genefpkm_valid), prefix='GC', index=genefpkm_valid.index)\n",
    "    clinical_valid = pd.concat([clinical_valid, gene_cluster_valid], axis=1)\n",
    "    \n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    x_train = clinical_train.join(genefpkm_train, how='inner').fillna(0).values\n",
    "    x_valid = clinical_valid.join(genefpkm_valid, how='inner').fillna(0).values\n",
    "    \n",
    "    #\n",
    "    # Denoising Autoencoder\n",
    "    #\n",
    "    print('Denoising autoencoder')\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    \n",
    "    dae = DenoisingAutoencoder(model_name='001_data_augmentation_adagrad_fold_{}'.format(i), summaries_dir='output/deep_models/')\n",
    "    \n",
    "    dae.build(n_inputs=x_train.shape[1], \n",
    "              encoder_units=(int(x_train.shape[1] * .9), int(x_train.shape[1] * .8), int(x_train.shape[1] * .7)), \n",
    "              decoder_units=(int(x_train.shape[1] * .8), int(x_train.shape[1] * .9)), \n",
    "              encoder_activation_function='relu', decoder_activation_function='relu')\n",
    "\n",
    "    dae.fit(x_train, batch_size=100, steps=10000, optimizer='adagrad', learning_rate=1e-6)\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
